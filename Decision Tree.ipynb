{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree is built from scratch, however I use some helper functions from other libraries: pandas for reading the data into a dataframe, random for shuffling the data before splitting, and scipy.stats for the entropy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import scipy.stats\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = pd.read_csv(\"https://raw.githubusercontent.com/ccalantzis/decision_tree/master/HTRU_2.csv\", error_bad_lines = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I split the data into the pulsars (data_1) and not pulsars (data_0). I then shuffle these individually, and take the first 90% of the shuffled data_0 and data_1, and use this for training data. The remaining 10% is for testing data. I split them individually so I have an equal proportion of positive samples in the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(range(original_data.shape[0]))\n",
    "\n",
    "data_0 = original_data.Class == 0\n",
    "labels_0 = [i for i in data_0.index if data_0[i]]\n",
    "data_1 = original_data.Class == 1\n",
    "labels_1 = [i for i in data_1.index if data_1[i]]\n",
    "\n",
    "random.seed(503)\n",
    "random.shuffle(labels_0)\n",
    "random.shuffle(labels_1)\n",
    "\n",
    "split_point_0 = int(0.9*len(labels_0))\n",
    "split_point_1 = int(0.9*len(labels_1))\n",
    "\n",
    "train_labels = labels_0[:split_point_0] + labels_1[:split_point_1]\n",
    "test_labels = labels_0[split_point_0:] + labels_1[split_point_1:]\n",
    "\n",
    "random.shuffle(train_labels)\n",
    "random.shuffle(test_labels)\n",
    "\n",
    "train_data = original_data.loc[train_labels].copy(deep=True)\n",
    "test_data = original_data.loc[test_labels].copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class Node represents any node in the decision tree which has children\n",
    "\n",
    "PredictionNode represents a node with no children i.e. it is a leaf node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode:\n",
    "    def __init__(self, attribute, threshold):\n",
    "        self.attribute = attribute\n",
    "        self.threshold = threshold\n",
    "    def addLeftChild(self, child):\n",
    "        self.left_child = child\n",
    "    def addRightChild(self, child):\n",
    "        self.right_child = child\n",
    "    def print_node(self):\n",
    "        print(\"Attribute: \", self.attribute)\n",
    "        print(\"Threshold: \", self.threshold)\n",
    "        \n",
    "class PredictionNode:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def prediction(self):\n",
    "        return self.value\n",
    "    def print_node(self):\n",
    "        print(\"Prediction: \", self.prediction)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split_column() creates a temporary column 'temp' based on the value of the provided column. If the value in the provided column is above the threshold value, the 'temp' value is 1, otherwise 0. This function is used in information_gain(), to calculate the information gain obtained from a particular variable split with a particular threshold value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_column(column_name, threshold, data):\n",
    "    data['temp'] = 0\n",
    "    filt = data[column_name] > threshold\n",
    "    data.loc[filt,'temp'] = 1\n",
    "    \n",
    "def information_gain(data):\n",
    "    entropy = scipy.stats.entropy(data.Class.value_counts())\n",
    "    if(data['Class'].mode()[0] == data['Class'].shape[0]):\n",
    "        return 0\n",
    "    filt = data['temp'] == 0\n",
    "    counts_0 = data.loc[filt, 'Class'].value_counts()\n",
    "    entropy_0 = scipy.stats.entropy(counts_0)\n",
    "    size_0 = data[filt].shape[0]\n",
    "    filt = data['temp'] == 1\n",
    "    counts_1 = data.loc[filt, 'Class'].value_counts()\n",
    "    entropy_1 = scipy.stats.entropy(counts_1)\n",
    "    size_1 = data[filt].shape[0]\n",
    "    orig_size = data.shape[0]\n",
    "\n",
    "    return entropy - ((size_0/orig_size)*entropy_0 + (size_1/orig_size)*entropy_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes one variable, finds 10 threshold values and calculates which threshold value produces the largest information gain for the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_info_gain_per_variable(column, data):\n",
    "    current_entropy = scipy.stats.entropy(data.Class.value_counts())\n",
    "    value_range = data[column].max() - data[column].min()\n",
    "    step = value_range/10\n",
    "    max_info_gain = 0\n",
    "    max_info_gain_threshold = 0\n",
    "    for i in range(1,10):\n",
    "        threshold = data[column].min() + i*step\n",
    "        split_column(column, threshold, data)\n",
    "        info_gain = information_gain(data)\n",
    "        if(info_gain > max_info_gain):\n",
    "            max_info_gain = info_gain\n",
    "            max_info_gain_threshold = threshold\n",
    "    return (max_info_gain, max_info_gain_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loops over all variables, finding the maximum information gain at any point in the decision tree. If the maximum information gain is below some minimum value, the function returns 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_info_gain_overall(data, entropy, used_columns, min_value = 0.005, vis = False):\n",
    "    max_info_gain = 0\n",
    "    max_info_gain_column = ''\n",
    "    max_info_gain_threshold = 0\n",
    "    \n",
    "    columns = [c for c in data.columns[:8] if c not in used_columns]\n",
    "    for column in columns:\n",
    "        result = max_info_gain_per_variable(column, data)\n",
    "        if (result[0] > max_info_gain):\n",
    "            max_info_gain = result[0]\n",
    "            max_info_gain_column = column\n",
    "            max_info_gain_threshold = result[1]\n",
    "    if(max_info_gain < min_value):\n",
    "        return 0\n",
    "    max_info_node = DecisionNode(max_info_gain_column, max_info_gain_threshold)\n",
    "    if vis:\n",
    "        plt.scatter(data[max_info_gain_column], data.Class)\n",
    "        plt.axvline(x=max_info_gain_threshold)\n",
    "        plt.show()\n",
    "    return max_info_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build_subtree is a recursive function which builds the decision tree in pre-order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_subtree(root, data, level, used_columns):\n",
    "    \n",
    "    # left node, or false node\n",
    "    filt = data[root.attribute] < root.threshold\n",
    "    data_subset = data.loc[filt].copy(deep=True)\n",
    "    entropy = scipy.stats.entropy(data_subset.Class.value_counts())\n",
    "    left_child = max_info_gain_overall(data_subset, entropy, used_columns)    \n",
    "    if(left_child == 0):\n",
    "        root.addLeftChild(PredictionNode(data_subset.Class.mode()[0]))\n",
    "    else:\n",
    "        root.addLeftChild(left_child) \n",
    "        level += 1\n",
    "        used_columns.append(left_child.attribute)\n",
    "        build_subtree(left_child, data_subset, level, used_columns)\n",
    "        level -= 1\n",
    "        used_columns.pop()\n",
    "        \n",
    "    # right node, or true node\n",
    "    filt = data[root.attribute] > root.threshold\n",
    "    data_subset = data.loc[filt].copy(deep=True)\n",
    "    entropy = scipy.stats.entropy(data_subset.Class.value_counts())\n",
    "    right_child = max_info_gain_overall(data_subset, entropy, used_columns)\n",
    "    if(right_child == 0):\n",
    "        root.addRightChild(PredictionNode(data_subset.Class.mode()[0]))\n",
    "    else:\n",
    "        root.addRightChild(right_child)\n",
    "        level += 1\n",
    "        used_columns.append(right_child.attribute)\n",
    "        build_subtree(right_child, data_subset, level, used_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check_subtree checks for redundant subtrees i.e. subtrees where every prediction node has the same value\n",
    "This function is used by build_ID3_tree to prune the redundant subtrees\n",
    "\n",
    "build_ID3_tree provides the root node and makes use of build_subtree to create the rest of the tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def check_subtree(node, values):\n",
    "    if(type(node) == PredictionNode):\n",
    "        values.append(node.value)\n",
    "    else:\n",
    "        check_subtree(node.left_child, values)\n",
    "        check_subtree(node.right_child, values)\n",
    "    return values\n",
    "\n",
    "def build_ID3_tree(data):\n",
    "    entropy = scipy.stats.entropy(data.Class.value_counts())\n",
    "    used_columns = []\n",
    "    root_node = max_info_gain_overall(data, entropy, used_columns)\n",
    "    used_columns.append(root_node.attribute)\n",
    "    build_subtree(root_node, data, 0, used_columns)\n",
    "\n",
    "    node_stack = []\n",
    "    node_stack.append(root_node)\n",
    "    node = root_node\n",
    "    while(len(node_stack) > 0):\n",
    "        if(type(node) != PredictionNode):\n",
    "            values = []\n",
    "            values = check_subtree(node.right_child, values)\n",
    "            if(len(set(values)) == 1):\n",
    "                node.addRightChild(PredictionNode(set(values).pop()))\n",
    "            else:\n",
    "                node_stack.append(node.right_child)\n",
    "            values = []\n",
    "            values = check_subtree(node.left_child, values)\n",
    "            if(len(set(values)) == 1):\n",
    "                node.addLeftChild(PredictionNode(set(values).pop()))                      \n",
    "            else:\n",
    "                node_stack.append(node.left_child)\n",
    "        node = node_stack.pop()\n",
    "    \n",
    "    return root_node\n",
    "        \n",
    "ID3_tree = build_ID3_tree(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualise_tree provides a visualisation of the tree structure using if-else statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " IF  Excess kurtosis of the integrated profile  <  1.1076487870999996\n",
      "\t THEN  0\n",
      " ELSE  Excess kurtosis of the integrated profile  >  1.1076487870999996\n",
      "\t IF  Skewness of the DM-SNR curve  <  100.04292962330001\n",
      "\t\t THEN  1\n",
      "\t ELSE  Skewness of the DM-SNR curve  >  100.04292962330001\n",
      "\t\t IF  Mean of the DM-SNR curve  <  2.5133779262000004\n",
      "\t\t\t THEN  0\n",
      "\t\t ELSE  Mean of the DM-SNR curve  >  2.5133779262000004\n",
      "\t\t\t THEN  1\n"
     ]
    }
   ],
   "source": [
    "def visualise_tree(node, level):\n",
    "    if(type(node.left_child) == PredictionNode):\n",
    "        print(level*\"\\t\", \"IF \", node.attribute, \" < \", node.threshold)\n",
    "        print((level+1)*\"\\t\", \"THEN \", node.left_child.value)\n",
    "    else:\n",
    "        print(level*\"\\t\", \"IF \", node.attribute, \" < \", node.threshold)\n",
    "        level += 1\n",
    "        visualise_tree(node.left_child, level)\n",
    "        level -= 1\n",
    "        \n",
    "    if(type(node.right_child) == PredictionNode):\n",
    "        print(level*\"\\t\", \"ELSE \", node.attribute, \" > \", node.threshold)\n",
    "        print((level+1)*\"\\t\", \"THEN \", node.right_child.value)\n",
    "    else:\n",
    "        print(level*\"\\t\", \"ELSE \", node.attribute, \" > \", node.threshold)\n",
    "        level += 1\n",
    "        visualise_tree(node.right_child, level)\n",
    "        \n",
    "visualise_tree(ID3_tree, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(node, data_input):\n",
    "    while(type(node) != PredictionNode):\n",
    "        if data_input[node.attribute] > node.threshold:\n",
    "            node = node.right_child\n",
    "        else:\n",
    "            node = node.left_child\n",
    "    return node.prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the accuracy of the ID3_tree on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy:  0.9743016759776536\n"
     ]
    }
   ],
   "source": [
    "real_values = [val for val in test_data.Class]\n",
    "pred_values = []\n",
    "for i in test_labels:\n",
    "    pred = make_prediction(ID3_tree, test_data.loc[i])\n",
    "    pred_values.append(pred)\n",
    "correct = len([1 for r,p in zip(real_values, pred_values) if (r==p)]) / float(len(pred_values))\n",
    "print(\"Test set accuracy: \", correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted     0    1\n",
      "Actual              \n",
      "0          1612   14\n",
      "1            32  132\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\"real\": real_values, \"pred\": pred_values}, columns=['real','pred'])\n",
    "\n",
    "confusion_matrix = pd.crosstab(df['real'], df['pred'], rownames=['Actual'], colnames=['Predicted'])\n",
    "print (confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
